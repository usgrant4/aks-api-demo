---
name: ci-cd
on:
  push:
    branches: [dev] # Added dev branch for testing
    paths:
      - "app/**"
      - "Dockerfile"
      - "kubernetes/**"
      - "helm/**"
      - "terraform/**"
      - ".github/workflows/ci-cd.yml"
  pull_request:
    paths:
      - "app/**"
      - "Dockerfile"
      - "kubernetes/**"
      - "helm/**"
      - "terraform/**"
  workflow_dispatch:
    inputs:
      action:
        description: "Action to perform"
        required: true
        default: "deploy"
        type: choice
        options:
          - "deploy"
          - "destroy"
      environment:
        description: "Target environment"
        required: true
        default: "dev"
        type: choice
        options:
          - "dev"
          - "prod"

env:
  TF_WORKING_DIR: terraform
  APP_NAME: aks-demo
  TF_BACKEND_RG: tfstate-rg
  TF_BACKEND_SA: tfstateugrantliatrio
  TF_BACKEND_CONTAINER: tfstate
  TF_BACKEND_LOCATION: westus2

jobs:
  build-test:
    if: github.event_name != 'workflow_dispatch' || github.event.inputs.action != 'destroy'
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps & run tests
        run: |
          pip install -r app/requirements.txt
          pytest -v --tb=short

  deploy:
    # Run on push to main/dev OR workflow_dispatch
    # When destroying, allow even if build-test was skipped
    if: |
      !cancelled() && (
        (github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/dev')) ||
        (github.event_name == 'workflow_dispatch' && (needs.build-test.result == 'success' || github.event.inputs.action == 'destroy')) ||
        (github.event_name == 'workflow_dispatch' && needs.build-test.result == 'skipped' && github.event.inputs.action == 'destroy')
      )
    needs: [build-test]
    runs-on: ubuntu-latest
    concurrency:
      group: ${{ github.event_name == 'workflow_dispatch' && format('{0}-deployment', github.event.inputs.environment) || (github.ref == 'refs/heads/main' && 'prod-deployment' || 'dev-deployment') }}
      cancel-in-progress: false
    permissions:
      id-token: write
      contents: read
      security-events: write
    steps:
      - uses: actions/checkout@v6

      - name: Set environment variables
        id: set_env
        run: |
          # Handle manual workflow dispatch
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            ENV="${{ github.event.inputs.environment }}"
            ACTION="${{ github.event.inputs.action }}"
            if [ "$ACTION" = "destroy" ]; then
              DESTROY="true"
            else
              DESTROY="false"
            fi
            echo "DESTROY_MODE=$DESTROY" >> $GITHUB_ENV
          else
            # Handle push events
            if [ "${{ github.ref }}" = "refs/heads/main" ]; then
              ENV="prod"
            else
              ENV="dev"
            fi
            echo "DESTROY_MODE=false" >> $GITHUB_ENV
          fi

          echo "ENVIRONMENT=$ENV" >> $GITHUB_ENV
          echo "TF_BACKEND_KEY=aks-demo-$ENV.tfstate" >> $GITHUB_ENV
          echo "DEPLOY_ENV=$ENV" >> $GITHUB_OUTPUT

          if [ "$DESTROY_MODE" = "true" ]; then
            echo "⚠️  DESTROY MODE: Will destroy $ENV environment"
          else
            echo "Deploying to environment: $ENV"
          fi

      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Setup Terraform backend
        env:
          RESOURCE_GROUP_NAME: ${{ env.TF_BACKEND_RG }}
          STORAGE_ACCOUNT_NAME: ${{ env.TF_BACKEND_SA }}
          CONTAINER_NAME: ${{ env.TF_BACKEND_CONTAINER }}
          LOCATION: ${{ env.TF_BACKEND_LOCATION }}
        run: |
          set -e
          set -o pipefail

          echo "Ensuring Terraform backend exists..."

          # Create resource group if it doesn't exist
          if ! az group show --name $RESOURCE_GROUP_NAME &>/dev/null; then
            echo "Creating resource group: $RESOURCE_GROUP_NAME"
            if ! az group create \
                --name $RESOURCE_GROUP_NAME \
                --location $LOCATION \
                --tags "purpose=terraform-state" "managed_by=github-actions"; then
              echo "ERROR: Failed to create resource group"
              exit 1
            fi
            echo "✓ Resource group created"
          else
            echo "✓ Resource group exists: $RESOURCE_GROUP_NAME"
          fi

          # Create storage account if it doesn't exist
          STORAGE_CREATED=false
          if ! az storage account show --name $STORAGE_ACCOUNT_NAME --resource-group $RESOURCE_GROUP_NAME &>/dev/null; then
            echo "Creating storage account: $STORAGE_ACCOUNT_NAME"
            if ! az storage account create \
                --name $STORAGE_ACCOUNT_NAME \
                --resource-group $RESOURCE_GROUP_NAME \
                --location $LOCATION \
                --sku Standard_LRS \
                --encryption-services blob \
                --https-only true \
                --min-tls-version TLS1_2 \
                --allow-blob-public-access false; then
              echo "ERROR: Failed to create storage account"
              exit 1
            fi
            STORAGE_CREATED=true
            echo "✓ Storage account created"
          else
            echo "✓ Storage account exists: $STORAGE_ACCOUNT_NAME"
          fi

          # Wait for storage account to be fully provisioned
          if [ "$STORAGE_CREATED" = true ]; then
            echo "Waiting for storage account to be fully ready..."
            for i in {1..30}; do
              PROVISIONING_STATE=$(az storage account show \
                --name $STORAGE_ACCOUNT_NAME \
                --resource-group $RESOURCE_GROUP_NAME \
                --query "provisioningState" -o tsv 2>/dev/null || echo "Unknown")

              if [ "$PROVISIONING_STATE" = "Succeeded" ]; then
                echo "✓ Storage account provisioning complete"
                break
              fi
              echo "Waiting for provisioning... ($i/30) State: $PROVISIONING_STATE"
              sleep 2
            done

            # Grant Storage Blob Data Contributor permission to service principal
            echo "Granting Storage Blob Data Contributor role to service principal..."
            SP_CLIENT_ID="${{ secrets.AZURE_CLIENT_ID }}"
            STORAGE_SCOPE="/subscriptions/${{ secrets.AZURE_SUBSCRIPTION_ID }}/resourceGroups/$RESOURCE_GROUP_NAME/providers/Microsoft.Storage/storageAccounts/$STORAGE_ACCOUNT_NAME"

            if az role assignment create \
                --assignee $SP_CLIENT_ID \
                --role "Storage Blob Data Contributor" \
                --scope "$STORAGE_SCOPE" 2>/dev/null; then
              echo "✓ Storage Blob Data Contributor role granted"
            else
              echo "ℹ Role assignment may already exist or permissions already granted"
            fi

            # Additional wait for backend availability and role propagation
            echo "Waiting 10s for backend stabilization and role propagation..."
            sleep 10
          fi

          # Create blob container if it doesn't exist
          if ! az storage container show \
              --name $CONTAINER_NAME \
              --account-name $STORAGE_ACCOUNT_NAME \
              --auth-mode login &>/dev/null; then
            echo "Creating blob container: $CONTAINER_NAME"
            if ! az storage container create \
                --name $CONTAINER_NAME \
                --account-name $STORAGE_ACCOUNT_NAME \
                --auth-mode login; then
              echo "ERROR: Failed to create blob container"
              exit 1
            fi
            echo "✓ Blob container created"
          else
            echo "✓ Blob container exists: $CONTAINER_NAME"
          fi

          # Validate access to the backend
          echo "Validating backend access..."
          if az storage blob list \
              --container-name $CONTAINER_NAME \
              --account-name $STORAGE_ACCOUNT_NAME \
              --auth-mode login \
              --num-results 1 &>/dev/null; then
            echo "✓ Backend access validated"
          else
            echo "ERROR: Cannot access storage container after granting permissions"
            echo "This may indicate a role propagation delay. Waiting additional 15s..."
            sleep 15

            # Try one more time
            if az storage blob list \
                --container-name $CONTAINER_NAME \
                --account-name $STORAGE_ACCOUNT_NAME \
                --auth-mode login \
                --num-results 1 &>/dev/null; then
              echo "✓ Backend access validated (after retry)"
            else
              echo "ERROR: Still cannot access storage"
              echo "Please check that service principal has necessary permissions"
              exit 1
            fi
          fi

          echo "✓ Terraform backend ready"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.10.5
          terraform_wrapper: false

      - name: Terraform Init
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          ARM_USE_OIDC: true
          ARM_BACKEND_RESOURCE_GROUP: ${{ env.TF_BACKEND_RG }}
          ARM_BACKEND_STORAGE_ACCOUNT: ${{ env.TF_BACKEND_SA }}
          ARM_BACKEND_CONTAINER: ${{ env.TF_BACKEND_CONTAINER }}
          ARM_BACKEND_KEY: ${{ env.TF_BACKEND_KEY }}
          TF_VAR_environment: ${{ env.ENVIRONMENT }}
        run: |
          # Backend configuration provided via environment variables
          # Terraform will use OIDC authentication via ARM_USE_OIDC=true
          terraform init \
            -backend-config="resource_group_name=$ARM_BACKEND_RESOURCE_GROUP" \
            -backend-config="storage_account_name=$ARM_BACKEND_STORAGE_ACCOUNT" \
            -backend-config="container_name=$ARM_BACKEND_CONTAINER" \
            -backend-config="key=$ARM_BACKEND_KEY" \
            -backend-config="use_oidc=true" \
            -input=false

      - name: Check and handle state lock
        if: always()
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          ARM_USE_OIDC: true
          TF_VAR_environment: ${{ env.ENVIRONMENT }}
        run: |
          echo "Checking for stale state locks..."

          # Try to get current lock info using Azure Storage API
          LOCK_BLOB="${{ env.TF_BACKEND_KEY }}.lock"
          LOCK_INFO=$(az storage blob show \
            --container-name ${{ env.TF_BACKEND_CONTAINER }} \
            --name "$LOCK_BLOB" \
            --account-name ${{ env.TF_BACKEND_SA }} \
            --auth-mode login \
            -o json 2>/dev/null || echo "{}")

          if [ "$LOCK_INFO" != "{}" ]; then
            echo "⚠️  Lock file found: $LOCK_BLOB"

            # Get lock metadata
            LOCK_CREATED=$(echo "$LOCK_INFO" | jq -r '.properties.createdOn // empty')

            if [ -n "$LOCK_CREATED" ]; then
              LOCK_AGE_SECONDS=$(( $(date +%s) - $(date -d "$LOCK_CREATED" +%s 2>/dev/null || echo "0") ))
              LOCK_AGE_MINUTES=$(( LOCK_AGE_SECONDS / 60 ))

              echo "Lock age: ${LOCK_AGE_MINUTES} minutes"

              # Consider locks older than 15 minutes as stale
              if [ $LOCK_AGE_MINUTES -gt 15 ]; then
                echo "⚠️  Lock appears stale (${LOCK_AGE_MINUTES} minutes old)"
                echo "Attempting to retrieve lock ID and force unlock..."

                # Download lock file to get the lock ID
                az storage blob download \
                  --container-name ${{ env.TF_BACKEND_CONTAINER }} \
                  --name "$LOCK_BLOB" \
                  --account-name ${{ env.TF_BACKEND_SA }} \
                  --auth-mode login \
                  --file /tmp/lock.json 2>/dev/null || true

                if [ -f /tmp/lock.json ]; then
                  LOCK_ID=$(jq -r '.ID // empty' /tmp/lock.json)
                  if [ -n "$LOCK_ID" ]; then
                    echo "Lock ID: $LOCK_ID"
                    echo "Force unlocking..."
                    terraform force-unlock -force "$LOCK_ID" || echo "Force unlock failed, will retry on terraform operation"
                  fi
                fi
              else
                echo "Lock is recent (${LOCK_AGE_MINUTES} minutes old), another operation may be in progress"
                echo "Waiting 30 seconds before proceeding..."
                sleep 30
              fi
            fi
          else
            echo "✓ No lock file found, state is unlocked"
          fi

      - name: Terraform Apply/Destroy
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          ARM_USE_OIDC: true
          TF_VAR_environment: ${{ env.ENVIRONMENT }}
        run: |
          if [ "${{ env.DESTROY_MODE }}" = "true" ]; then
            echo "⚠️  DESTROYING infrastructure for environment: ${{ env.ENVIRONMENT }}"
            echo "⚠️  This action is irreversible!"
            terraform destroy -auto-approve -input=false
            echo "✓ Infrastructure destroyed successfully"
          else
            echo "Deploying to environment: ${{ env.ENVIRONMENT }}"
            terraform apply -auto-approve -input=false
            echo "✓ Infrastructure deployed successfully"
          fi

      - name: Fetch outputs
        if: env.DESTROY_MODE != 'true'
        id: tfout
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          ARM_USE_OIDC: true
          TF_VAR_environment: ${{ env.ENVIRONMENT }}
        run: |
          ACR=$(terraform output -raw acr_login_server)
          echo "ACR_LOGIN_SERVER=$ACR" >> $GITHUB_OUTPUT
          ACR_NAME=$(echo $ACR | cut -d'.' -f1)
          echo "ACR_NAME=$ACR_NAME" >> $GITHUB_OUTPUT
          RG=$(terraform output -raw resource_group_name)
          echo "RG=$RG" >> $GITHUB_OUTPUT
          AKS=$(terraform output -raw aks_name)
          echo "AKS=$AKS" >> $GITHUB_OUTPUT

      - name: AKS get-credentials
        if: env.DESTROY_MODE != 'true'
        run: |
          retry() {
            local max_attempts=3
            local delay=5
            local attempt=1

            while [ $attempt -le $max_attempts ]; do
              if "$@"; then
                return 0
              fi
              echo "Attempt $attempt failed. Retrying in ${delay}s..."
              sleep $delay
              ((attempt++))
            done

            echo "Failed after $max_attempts attempts"
            return 1
          }

          retry az aks get-credentials \
            -g ${{ steps.tfout.outputs.RG }} \
            -n ${{ steps.tfout.outputs.AKS }} \
            --overwrite-existing

      - name: Build & push image with ACR build
        if: env.DESTROY_MODE != 'true'
        id: acr_build
        run: |
          VERSION="v1.0.${{ github.sha }}"
          echo "Building version: $VERSION"

          az acr build \
            --registry ${{ steps.tfout.outputs.ACR_NAME }} \
            --image "${{ env.APP_NAME }}:$VERSION" \
            --image "${{ env.APP_NAME }}:latest" \
            --platform linux/amd64 \
            --file Dockerfile \
            . \
            --output table

          echo "IMAGE_TAG=$VERSION" >> $GITHUB_OUTPUT

      - name: Verify image manifest
        if: env.DESTROY_MODE != 'true'
        run: |
          echo "Verifying image platform..."
          MANIFEST=$(az acr manifest show \
            --name "${{ env.APP_NAME }}:${{ steps.acr_build.outputs.IMAGE_TAG }}" \
            --registry ${{ steps.tfout.outputs.ACR_NAME }} \
            -o json)

          echo "$MANIFEST" | jq -r '.manifests[]? | "\(.platform.os)/\(.platform.architecture)"'

          # Check for unknown/unknown platforms
          UNKNOWN_COUNT=$(echo "$MANIFEST" | jq '[.manifests[]? | select(.platform.os == "unknown")] | length')
          if [ "$UNKNOWN_COUNT" != "0" ]; then
            echo "⚠️ Warning: Found $UNKNOWN_COUNT unknown platform manifests"
          else
            echo "✅ Clean manifest - all platforms valid"
          fi

      - name: Setup Helm
        if: env.DESTROY_MODE != 'true'
        uses: azure/setup-helm@v4
        with:
          version: 'v3.16.3'

      - name: Validate Helm chart
        if: env.DESTROY_MODE != 'true'
        run: |
          # Determine values file based on environment
          if [ "${{ env.ENVIRONMENT }}" == "prod" ]; then
            VALUES_FILE="./helm/aks-demo/values-prod.yaml"
          else
            VALUES_FILE="./helm/aks-demo/values.yaml"
          fi

          echo "Using values file: $VALUES_FILE"

          echo "Linting Helm chart..."
          helm lint ./helm/aks-demo \
            --values $VALUES_FILE

          echo "Validating Helm chart with dry-run..."
          helm install aks-demo ./helm/aks-demo \
            --dry-run --debug \
            --set image.registry=${{ steps.tfout.outputs.ACR_LOGIN_SERVER }} \
            --set image.tag=${{ github.sha }} \
            --set environment=${{ env.ENVIRONMENT }} \
            --values $VALUES_FILE \
            --namespace aks-demo-${{ env.ENVIRONMENT }} \
            > /tmp/helm-dry-run.yaml

          echo "✓ Helm chart validation passed"

          echo "Rendering templates for inspection..."
          helm template aks-demo ./helm/aks-demo \
            --set image.registry=${{ steps.tfout.outputs.ACR_LOGIN_SERVER }} \
            --set image.tag=${{ github.sha }} \
            --set environment=${{ env.ENVIRONMENT }} \
            --values $VALUES_FILE \
            > /tmp/helm-rendered.yaml

          echo "✓ Helm templates rendered successfully"

      - name: ACR login for Trivy scan
        if: env.DESTROY_MODE != 'true'
        run: |
          az acr login --name ${{ steps.tfout.outputs.ACR_NAME }}

      - name: Run Trivy vulnerability scanner
        if: env.DESTROY_MODE != 'true'
        uses: aquasecurity/trivy-action@0.33.1
        with:
          image-ref: '${{ steps.tfout.outputs.ACR_LOGIN_SERVER }}/${{ env.APP_NAME }}:${{ steps.acr_build.outputs.IMAGE_TAG }}'
          format: 'table'
          exit-code: '0'
          severity: 'CRITICAL,HIGH'
          ignore-unfixed: true

      - name: Run Trivy vulnerability scanner (SARIF output)
        if: env.DESTROY_MODE != 'true'
        uses: aquasecurity/trivy-action@0.33.1
        with:
          image-ref: '${{ steps.tfout.outputs.ACR_LOGIN_SERVER }}/${{ env.APP_NAME }}:${{ steps.acr_build.outputs.IMAGE_TAG }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          ignore-unfixed: true

      - name: Upload Trivy results to GitHub Security
        if: env.DESTROY_MODE != 'true'
        uses: github/codeql-action/upload-sarif@v4
        with:
          sarif_file: 'trivy-results.sarif'
          category: 'container-scan'

      - name: Check existing Helm release
        if: env.DESTROY_MODE != 'true'
        id: check_release
        continue-on-error: true
        run: |
          NAMESPACE="aks-demo-${{ env.ENVIRONMENT }}"

          # Create namespace if it doesn't exist
          kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

          if helm list -n $NAMESPACE | grep -q "aks-demo"; then
            echo "Existing Helm release found"
            echo "RELEASE_EXISTS=true" >> $GITHUB_OUTPUT

            # Get current release info
            helm status aks-demo -n $NAMESPACE

            # Save current revision for potential rollback
            CURRENT_REVISION=$(helm list -n $NAMESPACE -o json | jq -r '.[] | select(.name=="aks-demo") | .revision')
            echo "CURRENT_REVISION=$CURRENT_REVISION" >> $GITHUB_OUTPUT
            echo "Current revision: $CURRENT_REVISION"
          else
            echo "No existing Helm release found (initial deployment)"
            echo "RELEASE_EXISTS=false" >> $GITHUB_OUTPUT
          fi

      - name: Deploy with Helm
        if: env.DESTROY_MODE != 'true'
        id: helm_deploy
        run: |
          NAMESPACE="aks-demo-${{ env.ENVIRONMENT }}"
          ACR=${{ steps.tfout.outputs.ACR_LOGIN_SERVER }}
          IMAGE_TAG=${{ steps.acr_build.outputs.IMAGE_TAG }}

          # Determine values file based on environment
          if [ "${{ env.ENVIRONMENT }}" == "prod" ]; then
            VALUES_FILE="./helm/aks-demo/values-prod.yaml"
          else
            VALUES_FILE="./helm/aks-demo/values.yaml"
          fi

          echo "Deploying AKS Demo with Helm..."
          echo "Environment: ${{ env.ENVIRONMENT }}"
          echo "Namespace: $NAMESPACE"
          echo "Image: $ACR/${{ env.APP_NAME }}:$IMAGE_TAG"
          echo "Values file: $VALUES_FILE"

          helm upgrade --install aks-demo ./helm/aks-demo \
            --set image.registry=$ACR \
            --set image.tag=$IMAGE_TAG \
            --set environment=${{ env.ENVIRONMENT }} \
            --values $VALUES_FILE \
            --namespace $NAMESPACE \
            --create-namespace \
            --wait \
            --timeout 5m \
            --atomic \
            --cleanup-on-fail

          NEW_REVISION=$(helm list -n $NAMESPACE -o json | jq -r '.[] | select(.name=="aks-demo") | .revision')
          echo "NEW_REVISION=$NEW_REVISION" >> $GITHUB_OUTPUT
          echo "✓ Deployment completed successfully (revision $NEW_REVISION)"

      - name: Verify Helm deployment
        if: env.DESTROY_MODE != 'true'
        run: |
          NAMESPACE="aks-demo-${{ env.ENVIRONMENT }}"

          echo "Verifying Helm release status..."
          helm status aks-demo -n $NAMESPACE

          echo "Verifying all pods are ready..."
          DEPLOYMENT_NAME=$(kubectl get deployment -n $NAMESPACE -l app.kubernetes.io/name=aks-demo -o jsonpath='{.items[0].metadata.name}')

          # Get replica count from deployment
          DESIRED=$(kubectl get deploy $DEPLOYMENT_NAME -n $NAMESPACE -o jsonpath='{.spec.replicas}')
          READY=$(kubectl get deploy $DEPLOYMENT_NAME -n $NAMESPACE -o jsonpath='{.status.readyReplicas}')

          if [ "$READY" != "$DESIRED" ]; then
            echo "ERROR: Expected $DESIRED ready pods, got $READY"
            kubectl get pods -n $NAMESPACE -l app.kubernetes.io/name=aks-demo
            kubectl describe pods -n $NAMESPACE -l app.kubernetes.io/name=aks-demo
            kubectl logs -n $NAMESPACE -l app.kubernetes.io/name=aks-demo --tail=50
            exit 1
          fi

          echo "✓ All $READY/$DESIRED pods ready"
          kubectl get pods -n $NAMESPACE -l app.kubernetes.io/name=aks-demo

          echo "Verifying resources deployed by Helm..."
          kubectl get all,vpa,pdb -n $NAMESPACE -l app.kubernetes.io/instance=aks-demo

      - name: Test Helm rollback capability
        if: env.DESTROY_MODE != 'true' && steps.check_release.outputs.RELEASE_EXISTS == 'true'
        continue-on-error: true
        run: |
          NAMESPACE="aks-demo-${{ env.ENVIRONMENT }}"

          echo "Testing rollback capability (dry-run)..."
          helm rollback aks-demo ${{ steps.check_release.outputs.CURRENT_REVISION }} \
            -n $NAMESPACE \
            --dry-run

          echo "✓ Rollback capability verified"

      - name: Smoke test
        if: env.DESTROY_MODE != 'true'
        run: |
          NAMESPACE="aks-demo-${{ env.ENVIRONMENT }}"

          echo "Waiting for LoadBalancer IP..."
          SVC_NAME=$(kubectl get svc -n $NAMESPACE -l app.kubernetes.io/name=aks-demo -o jsonpath='{.items[0].metadata.name}')

          for i in $(seq 1 30); do
            IP=$(kubectl get svc $SVC_NAME -n $NAMESPACE \
              -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
            if [ -n "$IP" ]; then
              echo "✅ Service IP assigned: $IP"
              echo "Waiting 5s for service to be ready..."
              sleep 5

              echo "Testing main endpoint..."
              if curl -sS -f http://$IP/ \
                  --connect-timeout 10 \
                  --retry 3 \
                  --retry-delay 2; then
                echo ""

                echo "Testing health endpoint..."
                if curl -sS -f http://$IP/health \
                    --connect-timeout 10 \
                    --retry 3 \
                    --retry-delay 2; then
                  echo ""
                  echo "=========================================="
                  echo "  ✅ HELM DEPLOYMENT SUCCESSFUL!"
                  echo "=========================================="
                  echo ""
                  echo "Environment: ${{ env.ENVIRONMENT }}"
                  echo "Namespace: $NAMESPACE"
                  echo "API Endpoint: http://$IP/"
                  echo "Health Check: http://$IP/health"
                  echo "Image Tag: ${{ steps.acr_build.outputs.IMAGE_TAG }}"
                  echo "Helm Revision: ${{ steps.helm_deploy.outputs.NEW_REVISION }}"
                  echo ""
                  echo "Helm Commands:"
                  echo "  Status: helm status aks-demo -n $NAMESPACE"
                  echo "  History: helm history aks-demo -n $NAMESPACE"
                  echo "  Rollback: helm rollback aks-demo [REVISION] -n $NAMESPACE"
                  exit 0
                fi
              fi

              echo "⚠️ API not responding"
              kubectl get pods -n $NAMESPACE -l app.kubernetes.io/name=aks-demo
              kubectl logs -n $NAMESPACE -l app.kubernetes.io/name=aks-demo --tail=20
              exit 1
            fi
            echo "⏳ Waiting for IP (attempt $i/30)..."
            sleep 10
          done
          echo "✗ LoadBalancer IP not assigned"
          kubectl describe svc $SVC_NAME -n $NAMESPACE
          exit 1
