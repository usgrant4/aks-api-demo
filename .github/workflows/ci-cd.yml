---
name: ci-cd
on:
  push:
    branches: [dev] # Added dev branch for testing
    paths:
      - "app/**"
      - "Dockerfile"
      - "kubernetes/**"
      - "terraform/**"
      - ".github/workflows/ci-cd.yml"
  pull_request:
    paths:
      - "app/**"
      - "Dockerfile"
      - "kubernetes/**"
      - "terraform/**"
  workflow_dispatch:
    inputs:
      action:
        description: "Action to perform"
        required: true
        default: "deploy"
        type: choice
        options:
          - "deploy"
          - "destroy"
      environment:
        description: "Target environment"
        required: true
        default: "dev"
        type: choice
        options:
          - "dev"
          - "prod"

env:
  TF_WORKING_DIR: terraform
  APP_NAME: liatrio-demo
  TF_BACKEND_RG: tfstate-rg
  TF_BACKEND_SA: tfstateugrantliatrio
  TF_BACKEND_CONTAINER: tfstate
  TF_BACKEND_LOCATION: westus2

jobs:
  build-test:
    if: github.event_name != 'workflow_dispatch' || github.event.inputs.action != 'destroy'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps & run tests
        run: |
          pip install -r app/requirements.txt
          pytest -v --tb=short

  deploy:
    # Run on push to main/dev OR workflow_dispatch
    # When destroying, allow even if build-test was skipped
    if: |
      !cancelled() && (
        (github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/dev')) ||
        (github.event_name == 'workflow_dispatch' && (needs.build-test.result == 'success' || github.event.inputs.action == 'destroy')) ||
        (github.event_name == 'workflow_dispatch' && needs.build-test.result == 'skipped' && github.event.inputs.action == 'destroy')
      )
    needs: [build-test]
    runs-on: ubuntu-latest
    concurrency:
      group: ${{ github.event_name == 'workflow_dispatch' && format('{0}-deployment', github.event.inputs.environment) || (github.ref == 'refs/heads/main' && 'prod-deployment' || 'dev-deployment') }}
      cancel-in-progress: false
    permissions:
      id-token: write
      contents: read
      security-events: write
    steps:
      - uses: actions/checkout@v4

      - name: Set environment variables
        id: set_env
        run: |
          # Handle manual workflow dispatch
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            ENV="${{ github.event.inputs.environment }}"
            ACTION="${{ github.event.inputs.action }}"
            if [ "$ACTION" = "destroy" ]; then
              DESTROY="true"
            else
              DESTROY="false"
            fi
            echo "DESTROY_MODE=$DESTROY" >> $GITHUB_ENV
          else
            # Handle push events
            if [ "${{ github.ref }}" = "refs/heads/main" ]; then
              ENV="prod"
            else
              ENV="dev"
            fi
            echo "DESTROY_MODE=false" >> $GITHUB_ENV
          fi

          echo "ENVIRONMENT=$ENV" >> $GITHUB_ENV
          echo "TF_BACKEND_KEY=liatrio-demo-$ENV.tfstate" >> $GITHUB_ENV
          echo "DEPLOY_ENV=$ENV" >> $GITHUB_OUTPUT

          if [ "$DESTROY_MODE" = "true" ]; then
            echo "⚠️  DESTROY MODE: Will destroy $ENV environment"
          else
            echo "Deploying to environment: $ENV"
          fi

      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Setup Terraform backend
        env:
          RESOURCE_GROUP_NAME: ${{ env.TF_BACKEND_RG }}
          STORAGE_ACCOUNT_NAME: ${{ env.TF_BACKEND_SA }}
          CONTAINER_NAME: ${{ env.TF_BACKEND_CONTAINER }}
          LOCATION: ${{ env.TF_BACKEND_LOCATION }}
        run: |
          set -e
          set -o pipefail

          echo "Ensuring Terraform backend exists..."

          # Create resource group if it doesn't exist
          if ! az group show --name $RESOURCE_GROUP_NAME &>/dev/null; then
            echo "Creating resource group: $RESOURCE_GROUP_NAME"
            if ! az group create \
                --name $RESOURCE_GROUP_NAME \
                --location $LOCATION \
                --tags "purpose=terraform-state" "managed_by=github-actions"; then
              echo "ERROR: Failed to create resource group"
              exit 1
            fi
            echo "✓ Resource group created"
          else
            echo "✓ Resource group exists: $RESOURCE_GROUP_NAME"
          fi

          # Create storage account if it doesn't exist
          STORAGE_CREATED=false
          if ! az storage account show --name $STORAGE_ACCOUNT_NAME --resource-group $RESOURCE_GROUP_NAME &>/dev/null; then
            echo "Creating storage account: $STORAGE_ACCOUNT_NAME"
            if ! az storage account create \
                --name $STORAGE_ACCOUNT_NAME \
                --resource-group $RESOURCE_GROUP_NAME \
                --location $LOCATION \
                --sku Standard_LRS \
                --encryption-services blob \
                --https-only true \
                --min-tls-version TLS1_2 \
                --allow-blob-public-access false; then
              echo "ERROR: Failed to create storage account"
              exit 1
            fi
            STORAGE_CREATED=true
            echo "✓ Storage account created"
          else
            echo "✓ Storage account exists: $STORAGE_ACCOUNT_NAME"
          fi

          # Wait for storage account to be fully provisioned
          if [ "$STORAGE_CREATED" = true ]; then
            echo "Waiting for storage account to be fully ready..."
            for i in {1..30}; do
              PROVISIONING_STATE=$(az storage account show \
                --name $STORAGE_ACCOUNT_NAME \
                --resource-group $RESOURCE_GROUP_NAME \
                --query "provisioningState" -o tsv 2>/dev/null || echo "Unknown")

              if [ "$PROVISIONING_STATE" = "Succeeded" ]; then
                echo "✓ Storage account provisioning complete"
                break
              fi
              echo "Waiting for provisioning... ($i/30) State: $PROVISIONING_STATE"
              sleep 2
            done

            # Grant Storage Blob Data Contributor permission to service principal
            echo "Granting Storage Blob Data Contributor role to service principal..."
            SP_CLIENT_ID="${{ secrets.AZURE_CLIENT_ID }}"
            STORAGE_SCOPE="/subscriptions/${{ secrets.AZURE_SUBSCRIPTION_ID }}/resourceGroups/$RESOURCE_GROUP_NAME/providers/Microsoft.Storage/storageAccounts/$STORAGE_ACCOUNT_NAME"

            if az role assignment create \
                --assignee $SP_CLIENT_ID \
                --role "Storage Blob Data Contributor" \
                --scope "$STORAGE_SCOPE" 2>/dev/null; then
              echo "✓ Storage Blob Data Contributor role granted"
            else
              echo "ℹ Role assignment may already exist or permissions already granted"
            fi

            # Additional wait for backend availability and role propagation
            echo "Waiting 10s for backend stabilization and role propagation..."
            sleep 10
          fi

          # Create blob container if it doesn't exist
          if ! az storage container show \
              --name $CONTAINER_NAME \
              --account-name $STORAGE_ACCOUNT_NAME \
              --auth-mode login &>/dev/null; then
            echo "Creating blob container: $CONTAINER_NAME"
            if ! az storage container create \
                --name $CONTAINER_NAME \
                --account-name $STORAGE_ACCOUNT_NAME \
                --auth-mode login; then
              echo "ERROR: Failed to create blob container"
              exit 1
            fi
            echo "✓ Blob container created"
          else
            echo "✓ Blob container exists: $CONTAINER_NAME"
          fi

          # Validate access to the backend
          echo "Validating backend access..."
          if az storage blob list \
              --container-name $CONTAINER_NAME \
              --account-name $STORAGE_ACCOUNT_NAME \
              --auth-mode login \
              --num-results 1 &>/dev/null; then
            echo "✓ Backend access validated"
          else
            echo "ERROR: Cannot access storage container after granting permissions"
            echo "This may indicate a role propagation delay. Waiting additional 15s..."
            sleep 15

            # Try one more time
            if az storage blob list \
                --container-name $CONTAINER_NAME \
                --account-name $STORAGE_ACCOUNT_NAME \
                --auth-mode login \
                --num-results 1 &>/dev/null; then
              echo "✓ Backend access validated (after retry)"
            else
              echo "ERROR: Still cannot access storage"
              echo "Please check that service principal has necessary permissions"
              exit 1
            fi
          fi

          echo "✓ Terraform backend ready"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.10.5
          terraform_wrapper: false

      - name: Terraform Init/Apply/Destroy
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          ARM_USE_OIDC: true
          ARM_BACKEND_RESOURCE_GROUP: ${{ env.TF_BACKEND_RG }}
          ARM_BACKEND_STORAGE_ACCOUNT: ${{ env.TF_BACKEND_SA }}
          ARM_BACKEND_CONTAINER: ${{ env.TF_BACKEND_CONTAINER }}
          ARM_BACKEND_KEY: ${{ env.TF_BACKEND_KEY }}
          TF_VAR_environment: ${{ env.ENVIRONMENT }}
        run: |
          # Backend configuration provided via environment variables
          # Terraform will use OIDC authentication via ARM_USE_OIDC=true
          terraform init \
            -backend-config="resource_group_name=$ARM_BACKEND_RESOURCE_GROUP" \
            -backend-config="storage_account_name=$ARM_BACKEND_STORAGE_ACCOUNT" \
            -backend-config="container_name=$ARM_BACKEND_CONTAINER" \
            -backend-config="key=$ARM_BACKEND_KEY" \
            -backend-config="use_oidc=true" \
            -input=false

          if [ "${{ env.DESTROY_MODE }}" = "true" ]; then
            echo "⚠️  DESTROYING infrastructure for environment: ${{ env.ENVIRONMENT }}"
            echo "⚠️  This action is irreversible!"

            if ! terraform destroy -auto-approve -input=false 2>&1 | tee destroy.log; then
              if grep -q "state blob is already locked" destroy.log; then
                LOCK_ID=$(grep -A 2 "Lock Info:" destroy.log | grep "ID:" | awk '{print $2}')
                echo "⚠️  State is locked (ID: $LOCK_ID). Breaking stale lock..."
                terraform force-unlock -force "$LOCK_ID" || echo "Could not force unlock"
                terraform destroy -auto-approve -input=false
              else
                exit 1
              fi
            fi
            echo "✓ Infrastructure destroyed successfully"
          else
            echo "Deploying to environment: ${{ env.ENVIRONMENT }}"

            if ! terraform apply -auto-approve -input=false 2>&1 | tee apply.log; then
              if grep -q "state blob is already locked" apply.log; then
                LOCK_ID=$(grep -A 2 "Lock Info:" apply.log | grep "ID:" | awk '{print $2}')
                echo "⚠️  State is locked (ID: $LOCK_ID). Breaking stale lock..."
                terraform force-unlock -force "$LOCK_ID" || echo "Could not force unlock"
                terraform apply -auto-approve -input=false
              else
                exit 1
              fi
            fi
            echo "✓ Infrastructure deployed successfully"
          fi

      - name: Fetch outputs
        if: env.DESTROY_MODE != 'true'
        id: tfout
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          ARM_USE_OIDC: true
          TF_VAR_environment: ${{ env.ENVIRONMENT }}
        run: |
          ACR=$(terraform output -raw acr_login_server)
          echo "ACR_LOGIN_SERVER=$ACR" >> $GITHUB_OUTPUT
          ACR_NAME=$(echo $ACR | cut -d'.' -f1)
          echo "ACR_NAME=$ACR_NAME" >> $GITHUB_OUTPUT
          RG=$(terraform output -raw resource_group_name)
          echo "RG=$RG" >> $GITHUB_OUTPUT
          AKS=$(terraform output -raw aks_name)
          echo "AKS=$AKS" >> $GITHUB_OUTPUT

      - name: AKS get-credentials
        if: env.DESTROY_MODE != 'true'
        run: |
          retry() {
            local max_attempts=3
            local delay=5
            local attempt=1

            while [ $attempt -le $max_attempts ]; do
              if "$@"; then
                return 0
              fi
              echo "Attempt $attempt failed. Retrying in ${delay}s..."
              sleep $delay
              ((attempt++))
            done

            echo "Failed after $max_attempts attempts"
            return 1
          }

          retry az aks get-credentials \
            -g ${{ steps.tfout.outputs.RG }} \
            -n ${{ steps.tfout.outputs.AKS }} \
            --overwrite-existing

      - name: Build & push image with ACR build
        if: env.DESTROY_MODE != 'true'
        id: acr_build
        run: |
          VERSION="v1.0.${{ github.sha }}"
          echo "Building version: $VERSION"

          az acr build \
            --registry ${{ steps.tfout.outputs.ACR_NAME }} \
            --image "${{ env.APP_NAME }}:$VERSION" \
            --image "${{ env.APP_NAME }}:latest" \
            --platform linux/amd64 \
            --file Dockerfile \
            . \
            --output table

          echo "IMAGE_TAG=$VERSION" >> $GITHUB_OUTPUT

      - name: Verify image manifest
        if: env.DESTROY_MODE != 'true'
        run: |
          echo "Verifying image platform..."
          MANIFEST=$(az acr manifest show \
            --name "${{ env.APP_NAME }}:${{ steps.acr_build.outputs.IMAGE_TAG }}" \
            --registry ${{ steps.tfout.outputs.ACR_NAME }} \
            -o json)

          echo "$MANIFEST" | jq -r '.manifests[]? | "\(.platform.os)/\(.platform.architecture)"'

          # Check for unknown/unknown platforms
          UNKNOWN_COUNT=$(echo "$MANIFEST" | jq '[.manifests[]? | select(.platform.os == "unknown")] | length')
          if [ "$UNKNOWN_COUNT" != "0" ]; then
            echo "⚠️ Warning: Found $UNKNOWN_COUNT unknown platform manifests"
          else
            echo "✅ Clean manifest - all platforms valid"
          fi

      - name: ACR login for Trivy scan
        if: env.DESTROY_MODE != 'true'
        run: |
          az acr login --name ${{ steps.tfout.outputs.ACR_NAME }}

      - name: Run Trivy vulnerability scanner
        if: env.DESTROY_MODE != 'true'
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: '${{ steps.tfout.outputs.ACR_LOGIN_SERVER }}/${{ env.APP_NAME }}:${{ steps.acr_build.outputs.IMAGE_TAG }}'
          format: 'table'
          exit-code: '0'
          severity: 'CRITICAL,HIGH'
          ignore-unfixed: true

      - name: Run Trivy vulnerability scanner (SARIF output)
        if: env.DESTROY_MODE != 'true'
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: '${{ steps.tfout.outputs.ACR_LOGIN_SERVER }}/${{ env.APP_NAME }}:${{ steps.acr_build.outputs.IMAGE_TAG }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          ignore-unfixed: true

      - name: Upload Trivy results to GitHub Security
        if: env.DESTROY_MODE != 'true'
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'
          category: 'container-scan'

      - name: Save current deployment for rollback
        if: env.DESTROY_MODE != 'true'
        id: save_deployment
        continue-on-error: true
        run: |
          if kubectl get deployment liatrio-demo &>/dev/null; then
            echo "Saving current deployment state..."
            kubectl get deployment liatrio-demo -o yaml > /tmp/previous-deployment.yaml
            echo "PREVIOUS_DEPLOYMENT_EXISTS=true" >> $GITHUB_OUTPUT

            # Check current health
            READY=$(kubectl get deploy liatrio-demo -o jsonpath='{.status.readyReplicas}' || echo "0")
            DESIRED=$(kubectl get deploy liatrio-demo -o jsonpath='{.spec.replicas}' || echo "0")

            if [ "$READY" != "$DESIRED" ]; then
              echo "WARNING: Current deployment is unhealthy ($READY/$DESIRED ready)"
              kubectl get pods -l app=liatrio-demo
            else
              echo "✓ Current deployment is healthy ($READY/$DESIRED ready)"
            fi
          else
            echo "No existing deployment found (initial deployment)"
            echo "PREVIOUS_DEPLOYMENT_EXISTS=false" >> $GITHUB_OUTPUT
          fi

      - name: Deploy manifests
        if: env.DESTROY_MODE != 'true'
        id: deploy
        run: |
          ACR=${{ steps.tfout.outputs.ACR_LOGIN_SERVER }}
          IMAGE_TAG=${{ steps.acr_build.outputs.IMAGE_TAG }}

          # Use versioned tag for deployment
          sed "s#REPLACE_WITH_ACR_LOGIN/liatrio-demo:latest#$ACR/${{ env.APP_NAME }}:$IMAGE_TAG#" \
            kubernetes/deployment.yaml | kubectl apply -f -

          # Deploy VPA for automatic resource rightsizing
          echo "Deploying Vertical Pod Autoscaler..."
          kubectl apply -f kubernetes/vpa.yaml

      - name: Wait for rollout
        if: env.DESTROY_MODE != 'true'
        id: rollout
        run: |
          echo "Waiting for deployment rollout..."
          if ! kubectl rollout status deploy/liatrio-demo --timeout=300s; then
            echo "ERROR: Deployment rollout failed"

            # Attempt rollback if previous deployment exists
            if [ -f /tmp/previous-deployment.yaml ]; then
              echo "Attempting to rollback to previous deployment..."
              kubectl apply -f /tmp/previous-deployment.yaml
              echo "Waiting for rollback to complete..."
              kubectl rollout status deploy/liatrio-demo --timeout=120s || true
            fi

            exit 1
          fi
          echo "✓ Deployment rollout completed successfully"

      - name: Verify pod readiness
        if: env.DESTROY_MODE != 'true'
        run: |
          echo "Verifying all pods are ready..."

          # Get replica count from deployment
          DESIRED=$(kubectl get deploy liatrio-demo -o jsonpath='{.spec.replicas}')
          READY=$(kubectl get deploy liatrio-demo -o jsonpath='{.status.readyReplicas}')

          if [ "$READY" != "$DESIRED" ]; then
            echo "ERROR: Expected $DESIRED ready pods, got $READY"
            kubectl get pods -l app=liatrio-demo
            kubectl describe pods -l app=liatrio-demo
            kubectl logs -l app=liatrio-demo --tail=50
            exit 1
          fi

          echo "✓ All $READY/$DESIRED pods ready"
          kubectl get pods -l app=liatrio-demo

      - name: Smoke test
        if: env.DESTROY_MODE != 'true'
        run: |
          echo "Waiting for LoadBalancer IP..."
          for i in $(seq 1 30); do
            SVC="liatrio-demo-svc"
            IP=$(kubectl get svc $SVC \
              -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
            if [ -n "$IP" ]; then
              echo "✅ Service IP assigned: $IP"
              echo "Waiting 5s for service to be ready..."
              sleep 5

              if curl -sS -f http://$IP/ \
                  --connect-timeout 10 \
                  --retry 3 \
                  --retry-delay 2; then
                echo ""
                echo "=========================================="
                echo "  ✅ DEPLOYMENT SUCCESSFUL!"
                echo "=========================================="
                echo ""
                echo "API Endpoint: http://$IP/"
                echo "Health Check: http://$IP/health"
                echo "Version: v1.0.${{ github.sha }}"
                exit 0
              else
                echo "⚠️ API not responding"
                kubectl get pods -l app=liatrio-demo
                kubectl logs -l app=liatrio-demo --tail=20
                exit 1
              fi
            fi
            echo "⏳ Waiting for IP (attempt $i/30)..."
            sleep 10
          done
          echo "✗ LoadBalancer IP not assigned"
          kubectl describe svc liatrio-demo-svc
          exit 1
